{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn import svm\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_excel('yourdata.xlsx')\ndf.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Feats that always same value\na=df.columns[df.nunique() <= 1]\nconstant_feat=list(a)\ndf=df.drop(columns=constant_feat)\nprint(constant_feat)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.groupby(['diagnosis']).size()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df1=df[(df[\"diagnosis\"]==2)]\ndf2=df[(df[\"diagnosis\"]==3)]\nprint(df1.shape)\nprint(df2.shape)\ndf=pd.concat((df1,df2))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "select_feat=['A1','A2', 'A3','A4','A5','A6','A7','A8','A9','B1','B2','B3','B4','B5_B6','B6_B7','B7_B9','B9_B11','B10_B12','C1','D1','D2','D3','D4','D5','E1','E2','E3','ADI_A50','ADI_A51','ADI_A57','ADI_A49','ADI_A62','ADI_A63','ADI_A64','ADI_A65','ADI_A52','ADI_A53','ADI_A54', 'ADI_A31','ADI_A55','ADI_A56','ADI_A58','ADI_A59','ADI_B42','ADI_B43','ADI_B44','ADI_B45','ADI_B47','ADI_B48','ADI_B61','ADI_B34','ADI_B35','ADI_B33','ADI_B36','ADI_B37','ADI_B38','ADI_C67','ADI_C68','ADI_C39','ADI_C70','ADI_C77','ADI_C78','ADI_C69','ADI_C71','ADI_D2','ADI_D9','ADI_D10','ADI_D86','ADI_D87']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df=df[select_feat]\ndf=df.fillna(df.mean())\ndf=df.fillna(df.mean())\ny=np.zeros((df.shape[0],))\nx=df.to_numpy()\ny[:df1.shape[0]]=0\ny[df1.shape[0]:]=1\ny=y.astype(int)\nx = 2*((x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0)))-1\nprint(x.shape)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "feat=df.columns.to_list()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def my_svm_select_param(x,y):\n    \n    clf=SVC(probability=True)\n    kf = KFold(n_splits=10,shuffle=True)\n    a=np.arange(0,len(x))\n    auc=0\n    f1=0\n    per=0\n    rec=0\n    acc=0\n    conf=np.zeros((2,2))\n    for train_index, test_index in kf.split(a):\n    # define the models\n        x_tr=x[train_index,:]\n        y_tr=y[train_index]\n        x_test=x[test_index,:]\n        y_test=y[test_index]\n        param_grid = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf','linear','poly']}\n \n        grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n        grid.fit(x_tr, y_tr)\n        y_pred = grid.predict(x_test)\n        conf=conf+metrics.confusion_matrix(y_test,y_pred)\n        f1=f1+metrics.f1_score(y_test,y_pred)\n        rec=rec+metrics.recall_score(y_test,y_pred)\n        per=per+metrics.precision_score(y_test,y_pred)\n        acc=acc+metrics.accuracy_score(y_test,y_pred)\n        #pred=grid.predict_proba(x_test,probability=True)\n        #fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n        #auc=auc+metrics.auc(fpr, tpr)\n        \n        \n        # print best parameter after tuning\n        print(grid.best_params_)\n \n        # print how our model looks after hyper-parameter tuning\n        print(grid.best_estimator_)\n\n    f1=f1/10\n    rec=rec/10\n    per=per/10\n    acc=acc/10\n    #auc=auc/10\n\n    c=np.sum(conf,axis=1)\n    a1=conf[0,0]/c[0]\n    a2=conf[0,1]/c[0]\n    a3=conf[1,0]/c[1]\n    a4=conf[1,1]/c[1]\n    conf_norm=np.array([[a1,a2],[a3,a4]])\n    \n    return (f1,rec,per,acc,conf_norm,y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "(f1,rec,per,acc,conf_norm,y) =my_svm_select_param(x,y)\nprint('\\n'+ 'f-score '+str(f1))\nprint('\\n'+ 'recall '+str(rec))\nprint('\\n'+'per ' +str(per))\nprint('\\n'+ 'acc '+str(acc))\nprint('\\n'+ 'confusion matrix '+'\\n')\nprint(conf_norm)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def my_svm(x,y):\n    kf = KFold(n_splits=10,shuffle=True)\n    a=np.arange(0,len(x))\n    auc=0\n    f1=0\n    per=0\n    rec=0\n    acc=0\n    conf=np.zeros((2,2))\n    for train_index, test_index in kf.split(a):\n    # define the models\n        x_tr=x[train_index,:]\n        y_tr=y[train_index]\n        x_test=x[test_index,:]\n        y_test=y[test_index]\n        clf = svm.SVC(kernel='rbf',probability=True)\n        clf.fit(x_tr, y_tr)\n        y_pred = clf.predict(x_test)\n        conf=conf+metrics.confusion_matrix(y_test,y_pred)\n        f1=f1+metrics.f1_score(y_test,y_pred)\n        rec=rec+metrics.recall_score(y_test,y_pred)\n        per=per+metrics.precision_score(y_test,y_pred)\n        acc=acc+metrics.accuracy_score(y_test,y_pred)\n        pred=clf.predict_proba(x_test)\n        fpr, tpr, thresholds = metrics.roc_curve(y_test, pred[:,1])\n        auc=auc+metrics.auc(fpr, tpr)\n\n    f1=f1/10\n    rec=rec/10\n    per=per/10\n    acc=acc/10\n    auc=auc/10\n\n    c=np.sum(conf,axis=1)\n    a1=conf[0,0]/c[0]\n    a2=conf[0,1]/c[0]\n    a3=conf[1,0]/c[1]\n    a4=conf[1,1]/c[1]\n    conf_norm=np.array([[a1,a2],[a3,a4]])\n    \n    return (f1,rec,per,acc,conf_norm,y,auc) ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "n_select=5\nfinal_answer=np.zeros((n_select,3))\n\n## First Feature\n\nn_feat=x.shape[1]\nacc=np.zeros((n_feat,1))\nauc=np.zeros((n_feat,1))\nfor i in range(n_feat):\n    \n    d=x[:,i]\n    d=np.reshape(d,(len(d),1))\n    \n    (f1,rec,per,acc1,conf_norm,y,auc1)=my_svm(d,y)\n    acc[i,0]=acc1\n    auc[i,0]=auc1\n    \nfinal_answer[0,0]=np.argmax(acc)\nfinal_answer[0,1]=np.max(acc)\nfinal_answer[0,2]=np.max(auc)\n# Others featurs \ni=0\nwhile (i<n_select-1):\n    \n    \n    # Forward \n    \n    acc=np.zeros((n_feat,1))\n    \n    for j in range(n_feat):\n        \n    \n        if  not np.in1d(j,final_answer[:i+1,0]):\n            \n                \n                index=final_answer[:i+1,0]\n                index=np.append(index,j)\n                index=index.astype(int)\n                d=x[:,index]\n                d=np.reshape(d,(len(d),i+2))\n    \n                (f1,rec,per,acc1,conf_norm,y,auc1)=my_svm(d,y)\n                acc[j,0]=acc1\n                auc[j,0]=auc1\n    \n    final_answer[i+1,0]=np.argmax(acc)\n    final_answer[i+1,1]=np.max(acc)\n    final_answer[i+1,2]=np.max(auc)\n    feat_forward=np.argmax(acc)\n    final_answer_back=final_answer[:i+1,0]\n    final_answer_back=np.delete(final_answer_back,i)\n    \n    #Backward\n    \n    for j in range(n_feat):\n        \n        acc=np.zeros((n_feat,1))\n        \n        if  not np.in1d(j,final_answer[:,0]):\n            \n                \n                index=final_answer_back\n                index=np.append(index,j)\n                index=index.astype(int)\n                d=x[:,index]\n                d=np.reshape(d,(len(d),i+1))\n    \n                (f1,rec,per,acc1,conf_norm,y,auc1)=my_svm(d,y)\n                acc[j,0]=acc1\n                auc[j,0]=auc1\n            \n        if final_answer[i,0]<np.max(acc):\n            \n            final_answer[i,0]=np.argmax(acc)\n            final_answer[i,1]=np.max(acc)\n            final_answer[i,2]=np.max(auc)\n            \n        \n        \n    \n    #backwards\n    \n    i=i+1",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "select=final_answer[:,0]\nselect=select.astype(int)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for i in range(n_select):\n    print('\\n  '+select_feat[select[i]]+'       '+ str(final_answer[i,1]))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport scipy.stats\n\n# AUC comparison adapted from\n# https://github.com/Netflix/vmaf/\ndef compute_midrank(x):\n    \"\"\"Computes midranks.\n    Args:\n       x - a 1D numpy array\n    Returns:\n       array of midranks\n    \"\"\"\n    J = np.argsort(x)\n    Z = x[J]\n    N = len(x)\n    T = np.zeros(N, dtype=np.float)\n    i = 0\n    while i < N:\n        j = i\n        while j < N and Z[j] == Z[i]:\n            j += 1\n        T[i:j] = 0.5*(i + j - 1)\n        i = j\n    T2 = np.empty(N, dtype=np.float)\n    # Note(kazeevn) +1 is due to Python using 0-based indexing\n    # instead of 1-based in the AUC formula in the paper\n    T2[J] = T + 1\n    return T2\n\n\ndef compute_midrank_weight(x, sample_weight):\n    \"\"\"Computes midranks.\n    Args:\n       x - a 1D numpy array\n    Returns:\n       array of midranks\n    \"\"\"\n    J = np.argsort(x)\n    Z = x[J]\n    cumulative_weight = np.cumsum(sample_weight[J])\n    N = len(x)\n    T = np.zeros(N, dtype=np.float)\n    i = 0\n    while i < N:\n        j = i\n        while j < N and Z[j] == Z[i]:\n            j += 1\n        T[i:j] = cumulative_weight[i:j].mean()\n        i = j\n    T2 = np.empty(N, dtype=np.float)\n    T2[J] = T\n    return T2\n\n\ndef fastDeLong(predictions_sorted_transposed, label_1_count):\n    \"\"\"\n    The fast version of DeLong's method for computing the covariance of\n    unadjusted AUC.\n    Args:\n       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n          sorted such as the examples with label \"1\" are first\n    Returns:\n       (AUC value, DeLong covariance)\n    Reference:\n     @article{sun2014fast,\n       title={Fast Implementation of DeLong's Algorithm for\n              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n       author={Xu Sun and Weichao Xu},\n       journal={IEEE Signal Processing Letters},\n       volume={21},\n       number={11},\n       pages={1389--1393},\n       year={2014},\n       publisher={IEEE}\n     }\n    \"\"\"\n    # Short variables are named as they are in the paper\n    m = label_1_count\n    n = predictions_sorted_transposed.shape[1] - m\n    positive_examples = predictions_sorted_transposed[:, :m]\n    negative_examples = predictions_sorted_transposed[:, m:]\n    k = predictions_sorted_transposed.shape[0]\n\n    tx = np.empty([k, m], dtype=np.float)\n    ty = np.empty([k, n], dtype=np.float)\n    tz = np.empty([k, m + n], dtype=np.float)\n    for r in range(k):\n        tx[r, :] = compute_midrank(positive_examples[r, :])\n        ty[r, :] = compute_midrank(negative_examples[r, :])\n        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n    v01 = (tz[:, :m] - tx[:, :]) / n\n    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n    sx = np.cov(v01)\n    sy = np.cov(v10)\n    delongcov = sx / m + sy / n\n    return aucs, delongcov\n\n\ndef calc_pvalue(aucs, sigma):\n    \"\"\"Computes log(10) of p-values.\n    Args:\n       aucs: 1D array of AUCs\n       sigma: AUC DeLong covariances\n    Returns:\n       log10(pvalue)\n    \"\"\"\n    l = np.array([[1, -1]])\n    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n\n\ndef compute_ground_truth_statistics(ground_truth, sample_weight=None):\n    assert np.array_equal(np.unique(ground_truth), [0, 1])\n    order = (-ground_truth).argsort()\n    label_1_count = int(ground_truth.sum())\n    if sample_weight is None:\n        ordered_sample_weight = None\n    else:\n        ordered_sample_weight = sample_weight[order]\n\n    return order, label_1_count, ordered_sample_weight\n\n\ndef delong_roc_variance(ground_truth, predictions):\n    \"\"\"\n    Computes ROC AUC variance for a single set of predictions\n    Args:\n       ground_truth: np.array of 0 and 1\n       predictions: np.array of floats of the probability of being class 1\n    \"\"\"\n    sample_weight = None\n    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n        ground_truth, sample_weight)\n    predictions_sorted_transposed = predictions[np.newaxis, order]\n    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n    return aucs[0], delongcov\n\n\ndef delong_roc_test(ground_truth, predictions_one, predictions_two):\n    \"\"\"\n    Computes log(p-value) for hypothesis that two ROC AUCs are different\n    Args:\n       ground_truth: np.array of 0 and 1\n       predictions_one: predictions of the first model,\n          np.array of floats of the probability of being class 1\n       predictions_two: predictions of the second model,\n          np.array of floats of the probability of being class 1\n    \"\"\"\n    sample_weight = None\n    order, label_1_count,ordered_sample_weight = compute_ground_truth_statistics(ground_truth)\n    predictions_sorted_transposed = np.vstack((predictions_one, predictions_two))[:, order]\n    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n    return calc_pvalue(aucs, delongcov)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def my_svm_delog(x1,x2,y):\n    kf = KFold(n_splits=10,shuffle=True)\n    a=np.arange(0,len(x))\n    y_pred1_1=np.zeros((1,))\n    y_pred2_2=np.zeros((1,))\n    y_truth=np.zeros((1,))\n    for train_index, test_index in kf.split(a):\n    # define the models\n        x_tr1=x1[train_index,:]\n        x_tr2=x2[train_index,:]\n\n        \n        x_test1=x1[test_index,:]\n        x_test2=x2[test_index,:]\n \n        \n        y_test=y[test_index]\n        y_tr=y[train_index]\n        \n        clf1 = svm.SVC(kernel='linear',probability=True)\n        clf1.fit(x_tr1, y_tr)\n        clf2 = svm.SVC(kernel='linear',probability=True)\n        clf2.fit(x_tr2, y_tr)\n\n        \n        y_pred1 = clf1.predict_proba(x_test1)\n        y_pred2 = clf2.predict_proba(x_test2)\n\n        y_pred1_1=np.concatenate((y_pred1_1,y_pred1[:,1]))\n        y_pred2_2=np.concatenate((y_pred2_2,y_pred2[:,1]))\n\n        y_truth=np.concatenate((y_truth,y_test))\n        \n    y1=y_pred1_1[1:]\n    y2=y_pred2_2[1:]\n\n    y_t=y_truth[1:]\n        \n    return (y1,y2,y_t)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y1,y2,y_t=my_svm_delog(x,x[:,select],y)\n\nt1=delong_roc_test(y_t,y1,y2)\nt1=10**(t1[0][0])\n\n\n\nfpr, tpr, thresholds = metrics.roc_curve(y_t ,y1)\nauc1=metrics.auc(fpr, tpr)\nrec1=metrics.recall_score(y_t,y1.round())\nper1=metrics.precision_score(y_t,y1.round())\n\nfpr, tpr, thresholds = metrics.roc_curve(y_t, y2)\nauc2=metrics.auc(fpr, tpr)\nrec2=metrics.recall_score(y_t,y2.round())\nper2=metrics.precision_score(y_t,y2.round())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print('\\n'+'p-vlue for best -features vs all features '+str(t1))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print('\\n'+'p-vlue for best -features vs all features '+str(t1))\n\nprint('\\n'+'for all features AUC, Sensitivity and specificity {}  {} {}' .format(str(auc1),str(rec1),str(per1)))\nprint('\\n'+'for selected features AUC, Sensitivity and specificity {}  {} {}' .format(str(auc2),str(rec2),str(per2)))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}